{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMQPmCEiMtI3sjOoGR9YgvN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srinath-96/SubRedditAgent/blob/main/AdvancedRAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGytchYrR7sq",
        "outputId": "15b4cbeb-046f-4ddc-fd0c-dcc5a7c2835d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m746.6/746.6 kB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m76.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# Make sure necessary libraries are installed\n",
        "!pip install -q google-adk google-generativeai langchain langchain-google-genai chromadb jq pydantic_core tiktoken\n",
        "# tiktoken is often needed by LangChain text splitters"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain_community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qI0g25TzURna",
        "outputId": "ea3fd720-ec51-490e-d6ae-d795960b26b6"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.23-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.56 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.56)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.24 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.24)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.1.2)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.9.1)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.38)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (1.26.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.24->langchain_community) (0.3.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.24->langchain_community) (2.11.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.56->langchain_community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.56->langchain_community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.56->langchain_community) (4.13.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (3.10.17)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.23.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.1.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2025.4.26)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.2.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.56->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.24->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.24->langchain_community) (2.33.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.3.1)\n",
            "Downloading langchain_community-0.3.23-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, dataclasses-json, langchain_community\n",
            "Successfully installed dataclasses-json-0.6.7 langchain_community-0.3.23 marshmallow-3.26.1 mypy-extensions-1.1.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "from google.adk.agents import Agent\n",
        "from google.adk.tools import FunctionTool\n",
        "from google.adk.sessions import InMemorySessionService\n",
        "from google.adk.runners import Runner\n",
        "from google.genai import types as adk_types\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import asyncio\n",
        "import traceback"
      ],
      "metadata": {
        "id": "qemV9eOASLk6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LangChain Imports\n",
        "from langchain_community.document_loaders import JSONLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "# --- Imports for Parent Document Retriever ---\n",
        "from langchain.retrievers import ParentDocumentRetriever\n",
        "from langchain.storage import InMemoryStore # Simple in-memory store for parent docs\n",
        "# --- End Imports for Parent Document Retriever ---"
      ],
      "metadata": {
        "id": "SlsxPmH2ZR8e"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "try:\n",
        "\n",
        "\n",
        "  if 'GOOGLE_API_KEY' not in os.environ:\n",
        "\n",
        "      from google.colab import userdata\n",
        "      os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "  if not os.environ.get('GOOGLE_API_KEY'):\n",
        "      print(\"ERROR: GOOGLE_API_KEY not found.\")\n",
        "      print(\"Please set it using Colab Secrets (recommended) or by uncommenting and\")\n",
        "      print(\"replacing 'YOUR_API_KEY' in the code above.\")\n",
        "\n",
        "  else:\n",
        "      genai.configure(api_key=os.environ['GOOGLE_API_KEY'])\n",
        "      print(\"Google API Key configured successfully.\")\n",
        "\n",
        "except ImportError:\n",
        "    print(\"Could not import google.colab.userdata. Are you running in Colab?\")\n",
        "    if not os.environ.get('GOOGLE_API_KEY'):\n",
        "        print(\"ERROR: GOOGLE_API_KEY not found. Please set the environment variable.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during API key configuration: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78ZHzmMmShzk",
        "outputId": "86a8a474-9488-42e4-cac9-7eecc23fe549"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google API Key configured successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from google.colab import files\n",
        "\n",
        "print(\"Please upload your Reddit posts JSON file:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "json_filename = None\n",
        "if uploaded:\n",
        "    json_filename = list(uploaded.keys())[0]\n",
        "    print(f\"\\nUploaded file: '{json_filename}'\")\n",
        "else:\n",
        "    print(\"No file uploaded. Cannot proceed with data loading.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "qbj88WScXPUC",
        "outputId": "8e22f2d2-8cdf-442f-f0af-4f678ac47536"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please upload your Reddit posts JSON file:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-8f8c20cd-2c94-4e26-af5e-54ab2f8cd0b9\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-8f8c20cd-2c94-4e26-af5e-54ab2f8cd0b9\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving wallstreetbets_week_250posts_20250425_183349.json to wallstreetbets_week_250posts_20250425_183349.json\n",
            "\n",
            "Uploaded file: 'wallstreetbets_week_250posts_20250425_183349.json'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Global variables for retriever components\n",
        "vectorstore = None\n",
        "parent_store = None\n",
        "retriever = None\n",
        "embeddings = None # Define embeddings globally as well\n",
        "\n",
        "# Proceed only if the JSON file was uploaded and API key is set\n",
        "if json_filename and os.environ.get('GOOGLE_API_KEY'):\n",
        "    print(f\"\\n--- Setting up LangChain Parent Document Retriever for {json_filename} ---\")\n",
        "\n",
        "    try:\n",
        "        # 1. Define Metadata Extraction Function\n",
        "        def metadata_func(record: dict, metadata: dict) -> dict:\n",
        "            metadata[\"post_id\"] = record.get(\"id\", \"N/A\")\n",
        "            metadata[\"url\"] = record.get(\"url\", \"N/A\")\n",
        "            metadata[\"score\"] = record.get(\"score\", 0)\n",
        "            # Ensure source matches the filename for the loader\n",
        "            metadata[\"source\"] = json_filename # Add source filename to metadata\n",
        "            return metadata\n",
        "\n",
        "        # 2. Initialize JSONLoader\n",
        "        loader = JSONLoader(\n",
        "            file_path=json_filename,\n",
        "            jq_schema='.[ ] | {content: (\"Title: \" + .title + \"\\nBody: \" + (.body // \"\")), id: .id, url: .url, score: .score}',\n",
        "            content_key=\"content\",\n",
        "            metadata_func=metadata_func,\n",
        "            text_content=False\n",
        "        )\n",
        "\n",
        "        # 3. Load Documents\n",
        "        print(\"Loading documents via LangChain JSONLoader...\")\n",
        "        documents = loader.load()\n",
        "        # Filter out any potentially empty documents resulting from jq schema/source data\n",
        "        documents = [doc for doc in documents if doc.page_content.strip() and doc.page_content != \"Title: \\nBody:\"]\n",
        "        print(f\"Loaded and filtered {len(documents)} documents.\")\n",
        "\n",
        "        if documents:\n",
        "            # 4. Initialize Stores\n",
        "            print(\"Initializing parent docstore (InMemoryStore)...\")\n",
        "            parent_store = InMemoryStore() # Store for large parent chunks\n",
        "\n",
        "            print(\"Initializing Chroma vectorstore for child documents...\")\n",
        "            # Ensure embedding function is initialized\n",
        "            embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\") # Use appropriate model\n",
        "            vectorstore = Chroma(\n",
        "                collection_name=\"reddit_child_chunks_v2\", # Changed name slightly\n",
        "                embedding_function=embeddings\n",
        "                # persist_directory=\"./chroma_db_reddit_pdr\" # Optional: persist to disk\n",
        "            )\n",
        "\n",
        "            # 5. Initialize Splitters\n",
        "            # Larger chunks for parent documents (the context returned to LLM)\n",
        "            parent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\n",
        "            # Smaller chunks for child documents (used for embedding and search)\n",
        "            child_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=50)\n",
        "            print(\"Initialized parent and child text splitters.\")\n",
        "\n",
        "            # 6. Initialize ParentDocumentRetriever\n",
        "            print(\"Initializing ParentDocumentRetriever...\")\n",
        "            retriever = ParentDocumentRetriever(\n",
        "                vectorstore=vectorstore,    # Store for child vectors\n",
        "                docstore=parent_store,      # Store for parent documents\n",
        "                child_splitter=child_splitter,\n",
        "                parent_splitter=parent_splitter # Uses child_splitter if None, explicitly providing parent_splitter here\n",
        "            )\n",
        "\n",
        "            # 7. Add Documents to the Retriever (Using ids=None)\n",
        "            print(f\"Adding {len(documents)} documents to the retriever (this may take time)...\")\n",
        "\n",
        "            # --- MODIFIED LINE: Let the retriever generate IDs ---\n",
        "            retriever.add_documents(documents, ids=None, add_to_docstore=True)\n",
        "            # --- END MODIFICATION ---\n",
        "\n",
        "            # (The lines creating 'doc_ids' manually are no longer needed and can be removed/commented)\n",
        "            # # Optional: Provide unique IDs if available and desired (REMOVED)\n",
        "            # # doc_ids = [doc.metadata.get(\"post_id\", f\"doc_{i}\") for i, doc in enumerate(documents)] # REMOVED\n",
        "\n",
        "            print(\"Documents added successfully to ParentDocumentRetriever.\")\n",
        "            print(f\"Parent store size: {len(list(parent_store.yield_keys()))} keys added.\")\n",
        "            # You can try querying the vectorstore count if needed, e.g., vectorstore._collection.count() if using Chroma directly\n",
        "\n",
        "        else:\n",
        "            print(\"No documents loaded, skipping retriever setup.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during Parent Document Retriever setup: {e}\")\n",
        "        traceback.print_exc()\n",
        "        # Reset global vars on error to prevent issues later\n",
        "        vectorstore = None\n",
        "        parent_store = None\n",
        "        retriever = None\n",
        "        embeddings = None\n",
        "else:\n",
        "    if not json_filename:\n",
        "        print(\"Skipping LangChain setup: JSON file not uploaded.\")\n",
        "    else:\n",
        "        print(\"Skipping LangChain setup: Google API Key not configured.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHPN1cz9Ri1T",
        "outputId": "39c4e869-0d62-4d1a-c8b6-61cba510017e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Setting up LangChain Parent Document Retriever for wallstreetbets_week_250posts_20250425_183349.json ---\n",
            "Loading documents via LangChain JSONLoader...\n",
            "Loaded and filtered 250 documents.\n",
            "Initializing parent docstore (InMemoryStore)...\n",
            "Initializing Chroma vectorstore for child documents...\n",
            "Initialized parent and child text splitters.\n",
            "Initializing ParentDocumentRetriever...\n",
            "Adding 250 documents to the retriever (this may take time)...\n",
            "Documents added successfully to ParentDocumentRetriever.\n",
            "Parent store size: 313 keys added.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_context_parent_retriever(query: str) -> dict:\n",
        "    \"\"\"\n",
        "    Retrieves relevant PARENT document chunks using LangChain's\n",
        "    ParentDocumentRetriever, searching via child chunks.\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- ParentDocumentRetriever Function called with query: '{query}' ---\")\n",
        "    global retriever # Access the globally initialized retriever\n",
        "\n",
        "    if retriever is None:\n",
        "        print(\"  [Function Error] ParentDocumentRetriever is not initialized.\")\n",
        "        return {\"status\": \"error\", \"message\": \"Retriever not ready.\", \"context\": \"\"}\n",
        "\n",
        "    try:\n",
        "        # Use the retriever's get_relevant_documents method\n",
        "        print(f\"  Performing retrieval via ParentDocumentRetriever...\")\n",
        "        # This searches child vectors but returns corresponding parent documents\n",
        "        # Adjust 'k' inside the retriever definition if needed, default search args used here\n",
        "        retrieved_docs = retriever.get_relevant_documents(query) # Returns List[Document]\n",
        "\n",
        "        if not retrieved_docs:\n",
        "            print(\"  [Function Info] No relevant parent documents found.\")\n",
        "            return {\"status\": \"success\", \"message\": \"No relevant context found.\", \"context\": \"\"}\n",
        "        else:\n",
        "            # Format the retrieved PARENT documents' content into a single string\n",
        "            context_list = []\n",
        "            print(f\"  [Function Info] Retrieved {len(retrieved_docs)} parent document(s).\")\n",
        "            for i, doc in enumerate(retrieved_docs):\n",
        "                # Include metadata for better context understanding by the LLM and for tracing\n",
        "                context_chunk = (\n",
        "                    f\"--- Retrieved Context Chunk {i+1} ---\\n\"\n",
        "                    f\"Source Post ID: {doc.metadata.get('post_id', 'N/A')}\\n\"\n",
        "                    # f\"Source File: {doc.metadata.get('source', 'N/A')}\\n\" # Uncomment if loading from multiple files\n",
        "                    # f\"Source URL: {doc.metadata.get('url', 'N/A')}\\n\" # Optional\n",
        "                    f\"Content: {doc.page_content}\"\n",
        "                )\n",
        "                context_list.append(context_chunk)\n",
        "            context_string = \"\\n\\n\".join(context_list)\n",
        "\n",
        "            print(f\"  [Function Success] Formatted context from {len(retrieved_docs)} parent document(s).\")\n",
        "            # print(f\"DEBUG Context String:\\n{context_string[:500]}...\") # Uncomment for debugging\n",
        "            return {\"status\": \"success\", \"message\": \"Context retrieved.\", \"context\": context_string}\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  [Function Error] Error during ParentDocumentRetriever retrieval: {e}\")\n",
        "        traceback.print_exc()\n",
        "        return {\"status\": \"error\", \"message\": f\"An error occurred during retrieval: {str(e)}\", \"context\": \"\"}\n",
        "\n",
        "# --- Prepare the tool list for the ADK Agent ---\n",
        "# The agent's 'tools' argument expects a list of callable functions\n",
        "langchain_tool_list = []\n",
        "# Check if the 'retriever' object was successfully created in Cell 5\n",
        "if 'retriever' in globals() and retriever is not None:\n",
        "    if callable(retrieve_context_parent_retriever):\n",
        "        langchain_tool_list = [retrieve_context_parent_retriever]\n",
        "        print(\"\\nParentDocumentRetriever function prepared for ADK Agent.\")\n",
        "    else:\n",
        "        # This case should ideally not happen if the function definition is correct\n",
        "        print(\"\\nError: ParentDocumentRetriever function definition issue.\")\n",
        "else:\n",
        "    print(\"\\nWarning: ParentDocumentRetriever not ready, tool not prepared (check Cell 5 execution).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gn9EgcYSViU_",
        "outputId": "c8fb39d8-0813-4adf-a7b3-b5d76e48fb42"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ParentDocumentRetriever function prepared for ADK Agent.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model name for the ADK Agent\n",
        "ADK_AGENT_MODEL = 'gemini-2.0-flash' # Or 'gemini-1.5-flash-latest', 'gemini-1.5-pro-latest' etc.\n",
        "\n",
        "# Agent instructions - referencing the correct function name\n",
        "rag_agent_instruction = \"\"\"You are a factual Q&A assistant answering questions about specific Reddit posts. Your task is to answer user questions based *only* on the context provided by the 'retrieve_context_parent_retriever' function. Follow these steps precisely:\n",
        "1.  When you receive a user query, immediately call the 'retrieve_context_parent_retriever' function with the exact user query to find relevant information from the indexed Reddit posts. This function returns larger chunks of context.\n",
        "2.  Examine the result from the function, specifically the 'context' field in the returned dictionary. This field contains one or more chunks of text identified by '--- Retrieved Context Chunk X ---' and including metadata like 'Source Post ID'.\n",
        "3.  If the function returns context (status is 'success' and 'context' is not empty), synthesize an answer based *solely* on the information present in ALL the provided context chunks. Start your response with: \"Based on the retrieved context: \". Provide a detailed answer using the information from the context. Do not add any information not present in the retrieved context. You can reference Post IDs if helpful.\n",
        "4.  If the function returns a message indicating no relevant context was found (e.g., the 'context' field is empty or status is not 'success'), respond *exactly* with: \"I could not find relevant information about that in the indexed documents.\"\n",
        "5.  Do not use your general knowledge or any information outside the explicitly provided context from the function. If the function returns an error status, state that you encountered an issue retrieving information.\n",
        "\"\"\"\n",
        "\n",
        "rag_agent = None # Initialize to None\n",
        "# Check if API key is set AND the LangChain tool list is populated\n",
        "if os.environ.get('GOOGLE_API_KEY') and langchain_tool_list:\n",
        "    try:\n",
        "        rag_agent = Agent(\n",
        "            name=\"pdr_rag_agent_v1\", # Parent Document Retriever agent\n",
        "            model=ADK_AGENT_MODEL,\n",
        "            description=\"Answers questions using RAG based on Reddit posts, using LangChain's ParentDocumentRetriever via an ADK tool.\",\n",
        "            instruction=rag_agent_instruction,\n",
        "            tools=langchain_tool_list, # Pass the list containing the PDR retrieval function\n",
        "        )\n",
        "        print(f\"ADK Agent '{rag_agent.name}' created successfully with model '{ADK_AGENT_MODEL}'.\")\n",
        "        if rag_agent.tools:\n",
        "             # getattr(tool, '__name__', str(tool)) handles potential non-function items if list was wrong\n",
        "             print(f\"Agent Tools: {[getattr(tool, '__name__', str(tool)) for tool in rag_agent.tools]}\")\n",
        "        else:\n",
        "             print(\"Agent Tools: None\")\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR creating ADK Agent (Model: {ADK_AGENT_MODEL}): {e}\")\n",
        "        traceback.print_exc()\n",
        "else:\n",
        "    if not os.environ.get('GOOGLE_API_KEY'):\n",
        "        print(\"Agent creation skipped: Google API Key is not configured.\")\n",
        "    elif not langchain_tool_list:\n",
        "        print(\"Agent creation skipped: LangChain retrieval tool is not ready (check Cell 6).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tykNhfUYX69N",
        "outputId": "9dc92eaf-e1f9-4656-a2d7-ec1a62c0f912"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ADK Agent 'pdr_rag_agent_v1' created successfully with model 'gemini-2.0-flash'.\n",
            "Agent Tools: ['retrieve_context_parent_retriever']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adk_runner = None\n",
        "adk_session_service = None\n",
        "app_name_for_adk = \"PDR_LangChain_ADK_App\" # Define specific app name\n",
        "\n",
        "if rag_agent:\n",
        "    try:\n",
        "        adk_session_service = InMemorySessionService()\n",
        "        adk_runner = Runner(\n",
        "            agent=rag_agent,\n",
        "            app_name=app_name_for_adk, # Use defined app name\n",
        "            session_service=adk_session_service,\n",
        "        )\n",
        "        print(f\"ADK Runner and Session Service initialized for App: '{app_name_for_adk}'.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error initializing ADK Runner/SessionService: {e}\")\n",
        "        traceback.print_exc()\n",
        "        adk_runner = None\n",
        "        adk_session_service = None\n",
        "else:\n",
        "    print(\"ADK Runner initialization skipped because the Agent was not created.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F81jTRDQX8qB",
        "outputId": "c34426e7-95fe-42ac-87c6-c87c3b9d00de"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ADK Runner and Session Service initialized for App: 'PDR_LangChain_ADK_App'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Modified function to handle a single turn within an existing session\n",
        "async def handle_chat_turn(session_id: str, user_query: str):\n",
        "    \"\"\"\n",
        "    Sends a user query to the agent within a specific session and returns the response.\n",
        "    Assumes the session already exists.\n",
        "    \"\"\"\n",
        "    if not adk_runner or not adk_session_service:\n",
        "        print(\"ERROR: ADK Runner or Session Service is not initialized.\")\n",
        "        return \"Error: ADK components not ready.\"\n",
        "    if not rag_agent:\n",
        "        print(\"ERROR: ADK Agent is not initialized.\")\n",
        "        return \"Error: Agent not ready.\"\n",
        "\n",
        "    app_name = getattr(adk_runner, 'app_name', app_name_for_adk) # Get app name\n",
        "    user_id = \"colab_chat_user\" # Consistent user ID for the chat\n",
        "\n",
        "    print(f\"  Sending to Agent (Session: {session_id}): '{user_query}'\")\n",
        "    final_response = \"Agent did not provide a final text response.\"\n",
        "\n",
        "    try:\n",
        "        # Verify session exists before running (optional, but good check)\n",
        "        try:\n",
        "            adk_session_service.get_session(app_name=app_name, user_id=user_id, session_id=session_id)\n",
        "        except Exception as get_e:\n",
        "            print(f\"  ERROR: Cannot get session {session_id} before running turn: {get_e}\")\n",
        "            return f\"Error: Session {session_id} may not exist or became invalid.\"\n",
        "\n",
        "        # Format the user input\n",
        "        content = adk_types.Content(role='user', parts=[adk_types.Part(text=user_query)])\n",
        "\n",
        "        # Run the agent for this turn using the existing session_id\n",
        "        async for event in adk_runner.run_async(user_id=user_id, session_id=session_id, new_message=content):\n",
        "            event_type = type(event).__name__\n",
        "            # Optional: Reduce logging verbosity for chat interface\n",
        "            # print(f\"    [ADK Event]: {event_type}\")\n",
        "\n",
        "            if hasattr(event, 'tool_call') and event.tool_call:\n",
        "                 tool_name = getattr(event.tool_call, 'name', 'N/A')\n",
        "                 print(f\"      DEBUG: Agent calling tool: '{tool_name}'\") # Keep tool call log\n",
        "\n",
        "            if hasattr(event, 'content') and event.content:\n",
        "                content_role = getattr(event.content, 'role', 'N/A')\n",
        "                if hasattr(event.content, 'parts'):\n",
        "                    for part in event.content.parts:\n",
        "                        if hasattr(part, 'function_response') and part.function_response:\n",
        "                             # Optional: Log tool response success/failure\n",
        "                             fr_name = getattr(part.function_response, 'name', 'N/A')\n",
        "                             print(f\"      DEBUG: Got response from tool '{fr_name}'.\")\n",
        "                        if hasattr(part, 'text') and part.text:\n",
        "                             if content_role == 'model':\n",
        "                                 # Capture the final text response from the model for this turn\n",
        "                                 final_response = part.text\n",
        "                                 # No need to print partial model text here, we'll print final below\n",
        "\n",
        "        # print(f\"\\n--- Agent turn finished for Session: {session_id} ---\") # Reduce noise\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR during chat turn processing for session {session_id}: {e}\")\n",
        "        traceback.print_exc()\n",
        "        final_response = f\"An error occurred: {str(e)}\"\n",
        "    # NOTE: No session deletion here - the session persists for the chat\n",
        "\n",
        "    return final_response"
      ],
      "metadata": {
        "id": "fgojbCmsDF3Z"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def run_interactive_chat():\n",
        "    \"\"\"Runs a loop to chat with the RAG agent using a persistent session.\"\"\"\n",
        "    if not adk_runner or not adk_session_service:\n",
        "        print(\"ERROR: ADK components not ready. Cannot start chat.\")\n",
        "        return\n",
        "    if not rag_agent:\n",
        "        print(\"ERROR: ADK Agent not ready. Cannot start chat.\")\n",
        "        return\n",
        "\n",
        "    # --- Chat Setup ---\n",
        "    session_id = f\"persistent_chat_{os.urandom(6).hex()}\" # Single session ID for the whole chat\n",
        "    user_id = \"colab_chat_user\"\n",
        "    app_name = getattr(adk_runner, 'app_name', app_name_for_adk)\n",
        "    print(\"\\n--- Starting Interactive Chat ---\")\n",
        "    print(f\"Using Session ID: {session_id}\")\n",
        "    print('Type \"quit\" or \"exit\" to end the chat.')\n",
        "\n",
        "    # --- Create the persistent session ONCE ---\n",
        "    try:\n",
        "        print(f\"Creating chat session {session_id}...\")\n",
        "        adk_session_service.create_session(\n",
        "            app_name=app_name,\n",
        "            user_id=user_id,\n",
        "            session_id=session_id\n",
        "        )\n",
        "        print(\"Session created.\")\n",
        "    except Exception as create_e:\n",
        "        print(f\"FATAL ERROR: Could not create persistent session {session_id}: {create_e}\")\n",
        "        return # Cannot proceed without a session\n",
        "\n",
        "    # --- Chat Loop ---\n",
        "    while True:\n",
        "        try:\n",
        "            user_input = input(\"\\nYou: \")\n",
        "            if user_input.lower() in [\"quit\", \"exit\"]:\n",
        "                print(\"Ending chat.\")\n",
        "                break\n",
        "            if not user_input.strip():\n",
        "                continue\n",
        "\n",
        "            # Call the handler for this turn using the persistent session ID\n",
        "            agent_response = await handle_chat_turn(session_id, user_input)\n",
        "\n",
        "            print(f\"\\nAgent: {agent_response}\")\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\nEnding chat due to keyboard interrupt.\")\n",
        "            break\n",
        "        except Exception as loop_e:\n",
        "            print(f\"\\nAn error occurred in the chat loop: {loop_e}\")\n",
        "            traceback.print_exc()\n",
        "            # Decide whether to break or continue after an error\n",
        "            break\n",
        "\n",
        "    # --- Chat Cleanup (Optional) ---\n",
        "    try:\n",
        "        print(f\"\\nAttempting to delete chat session {session_id}...\")\n",
        "        adk_session_service.delete_session(\n",
        "            app_name=app_name,\n",
        "            user_id=user_id,\n",
        "            session_id=session_id\n",
        "        )\n",
        "        print(\"Chat session deleted.\")\n",
        "    except Exception as del_e:\n",
        "        print(f\"Info/Warning during chat session deletion: {del_e}\")\n",
        "\n",
        "# --- Run the Interactive Chat in Colab ---\n",
        "# Use await to run the async chat function directly in the notebook cell\n",
        "await run_interactive_chat()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JpLacWBdDF1J",
        "outputId": "2818fdce-4a87-459c-d44e-dce9b186e95b"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting Interactive Chat ---\n",
            "Using Session ID: persistent_chat_52b37a587cb4\n",
            "Type \"quit\" or \"exit\" to end the chat.\n",
            "Creating chat session persistent_chat_52b37a587cb4...\n",
            "Session created.\n",
            "\n",
            "You: What are the main themes in the documents?\n",
            "  Sending to Agent (Session: persistent_chat_52b37a587cb4): 'What are the main themes in the documents?'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'],returning concatenated text result from text parts,check out the non text parts for full response from model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- ParentDocumentRetriever Function called with query: 'What are the main themes in the documents?' ---\n",
            "  Performing retrieval via ParentDocumentRetriever...\n",
            "  [Function Info] Retrieved 3 parent document(s).\n",
            "  [Function Success] Formatted context from 3 parent document(s).\n",
            "      DEBUG: Got response from tool 'retrieve_context_parent_retriever'.\n",
            "\n",
            "Agent: Based on the retrieved context: The main themes in the documents revolve around trade war implications, particularly between the United States and China (Post ID: 1k4i5n9, 1k76tug), and potential investment opportunities arising from these tensions (Post ID: 1k4i5n9). Tariffs and their impact on specific sectors and companies are also a recurring theme, with companies like MP Materials being highlighted for their potential due to Western alignment and companies like Skechers facing challenges due to reliance on Asian manufacturing and reciprocal tariffs from China (Post ID: 1k4i5n9, 1k76tug). Another theme involves identifying potential market reversals and gains, particularly in options trading, based on political events and market reactions (Post ID: 1k5ih3m).\n",
            "\n",
            "\n",
            "You: What are some potential stock options that could yield good returns?\n",
            "  Sending to Agent (Session: persistent_chat_52b37a587cb4): 'What are some potential stock options that could yield good returns?'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'],returning concatenated text result from text parts,check out the non text parts for full response from model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- ParentDocumentRetriever Function called with query: 'What are some potential stock options that could yield good returns?' ---\n",
            "  Performing retrieval via ParentDocumentRetriever...\n",
            "  [Function Info] Retrieved 4 parent document(s).\n",
            "  [Function Success] Formatted context from 4 parent document(s).\n",
            "      DEBUG: Got response from tool 'retrieve_context_parent_retriever'.\n",
            "\n",
            "Agent: Based on the retrieved context:\n",
            "\n",
            "*   **BNTX options:** Buying short-dated BNTX options is suggested, anticipating a potential 20%+ stock increase based on results presented on April 27 and 29 (Post ID: 1k7s0er).\n",
            "*   **MP Materials:** The author holds a mixture of calls and shares in MP Materials, which have already begun to return sizable gains (Post ID: 1k4i5n9).\n",
            "*   **SPY calls:** Timing calls on SPY in anticipation of news breaking that the US & China have reached a trade deal is mentioned as a potential strategy (Post ID: 1k70nz2). The expectation is a 2%+ jump once the news breaks.\n",
            "*   **Alphabet (Google):** Selling covered calls for earnings may provide some extra income, if you already have the shares (Post ID: 1k70ero).\n",
            "\n",
            "\n",
            "You: what was my previous question\n",
            "  Sending to Agent (Session: persistent_chat_52b37a587cb4): 'what was my previous question'\n",
            "\n",
            "Agent: Based on the context, your previous question was: \"What are some potential stock options that could yield good returns?\"\n",
            "\n",
            "\n",
            "You: exit\n",
            "Ending chat.\n",
            "\n",
            "Attempting to delete chat session persistent_chat_52b37a587cb4...\n",
            "Chat session deleted.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VtmTImmzYCNb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}