{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNxoWVnR3GvrC3jCORGULCr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srinath-96/SubRedditAgent/blob/main/SimpleRAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGytchYrR7sq",
        "outputId": "f8594543-7d41-4f8e-cb2a-620d8a3a0338"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.1/232.1 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/95.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/217.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m217.1/217.1 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m334.1/334.1 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install -q google-adk google-generativeai numpy scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "from google.adk.agents import Agent\n",
        "from google.adk.tools import FunctionTool\n",
        "from google.adk.sessions import InMemorySessionService\n",
        "from google.adk.runners import Runner\n",
        "from google.genai import types as adk_types\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import asyncio\n",
        "import traceback"
      ],
      "metadata": {
        "id": "qemV9eOASLk6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SlsxPmH2ZR8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "try:\n",
        "\n",
        "\n",
        "  if 'GOOGLE_API_KEY' not in os.environ:\n",
        "\n",
        "      from google.colab import userdata\n",
        "      os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "  if not os.environ.get('GOOGLE_API_KEY'):\n",
        "      print(\"ERROR: GOOGLE_API_KEY not found.\")\n",
        "      print(\"Please set it using Colab Secrets (recommended) or by uncommenting and\")\n",
        "      print(\"replacing 'YOUR_API_KEY' in the code above.\")\n",
        "\n",
        "  else:\n",
        "      genai.configure(api_key=os.environ['GOOGLE_API_KEY'])\n",
        "      print(\"Google API Key configured successfully.\")\n",
        "\n",
        "except ImportError:\n",
        "    print(\"Could not import google.colab.userdata. Are you running in Colab?\")\n",
        "    if not os.environ.get('GOOGLE_API_KEY'):\n",
        "        print(\"ERROR: GOOGLE_API_KEY not found. Please set the environment variable.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during API key configuration: {e}\")\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78ZHzmMmShzk",
        "outputId": "62d7ebfe-0af2-4717-8492-05de609c32f5"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google API Key configured successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from google.colab import files\n",
        "\n",
        "print(\"Please upload your Reddit posts JSON file:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "json_filename = None\n",
        "if uploaded:\n",
        "    json_filename = list(uploaded.keys())[0]\n",
        "    print(f\"\\nUploaded file: '{json_filename}'\")\n",
        "else:\n",
        "    print(\"No file uploaded. Cannot proceed with data loading.\")\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "qbj88WScXPUC",
        "outputId": "2f497c95-75db-4a0b-ce58-37dee3e32b0c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please upload your Reddit posts JSON file:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b2cd5d23-7f74-46f1-ae01-c9b0d8254ebe\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b2cd5d23-7f74-46f1-ae01-c9b0d8254ebe\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving wallstreetbets_week_250posts_20250425_183349.json to wallstreetbets_week_250posts_20250425_183349 (1).json\n",
            "\n",
            "Uploaded file: 'wallstreetbets_week_250posts_20250425_183349 (1).json'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain_community langchain_text_splitters langchain_google_genai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "R3I9mWOAaoLk",
        "outputId": "f610bed7-962b-4e6b-871b-4315a1080c68"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.23-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: langchain_text_splitters in /usr/local/lib/python3.11/dist-packages (0.3.8)\n",
            "Collecting langchain_google_genai\n",
            "  Downloading langchain_google_genai-2.1.4-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.56 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.56)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.24 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.24)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.1.2)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.9.1)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.34)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.2)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from langchain_google_genai)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting google-ai-generativelanguage<0.7.0,>=0.6.18 (from langchain_google_genai)\n",
            "  Downloading google_ai_generativelanguage-0.6.18-py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_google_genai) (2.11.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (2.24.2)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (5.29.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.56->langchain_community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.56->langchain_community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.56->langchain_community) (4.13.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain_google_genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain_google_genai) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain_google_genai) (0.4.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2025.1.31)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.2.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (1.70.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (1.71.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (1.71.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (4.9.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.56->langchain_community) (3.0.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (0.6.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.3.1)\n",
            "Downloading langchain_community-0.3.23-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_google_genai-2.1.4-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading google_ai_generativelanguage-0.6.18-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: filetype, mypy-extensions, marshmallow, typing-inspect, dataclasses-json, google-ai-generativelanguage, langchain_google_genai, langchain_community\n",
            "  Attempting uninstall: google-ai-generativelanguage\n",
            "    Found existing installation: google-ai-generativelanguage 0.6.15\n",
            "    Uninstalling google-ai-generativelanguage-0.6.15:\n",
            "      Successfully uninstalled google-ai-generativelanguage-0.6.15\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.18 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dataclasses-json-0.6.7 filetype-1.2.0 google-ai-generativelanguage-0.6.18 langchain_community-0.3.23 langchain_google_genai-2.1.4 marshmallow-3.26.1 mypy-extensions-1.1.0 typing-inspect-0.9.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "8a135258d67b421891bd1c012e62a33e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install jq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3kcPj9QFbDZp",
        "outputId": "8f5791fc-553d-43dd-9821-6c06179be413"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jq\n",
            "  Downloading jq-1.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
            "Downloading jq-1.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (746 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/746.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m737.3/746.6 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m746.6/746.6 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jq\n",
            "Successfully installed jq-1.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install chromadb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aq5N3IXGa9O5",
        "outputId": "fe0f987f-a3eb-4120-9d27-218fa0e33b21"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting chromadb\n",
            "  Downloading chromadb-1.0.7-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.2.2.post1)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (2.11.3)\n",
            "Collecting chroma-hnswlib==0.7.6 (from chromadb)\n",
            "  Downloading chroma_hnswlib-0.7.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\n",
            "Collecting fastapi==0.115.9 (from chromadb)\n",
            "  Downloading fastapi-0.115.9-py3-none-any.whl.metadata (27 kB)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.34.2)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.11/dist-packages (from chromadb) (2.0.2)\n",
            "Collecting posthog>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-4.0.1-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.13.2)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.21.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.32.1)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.32.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.53b1-py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.32.1)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.21.1)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.67.1)\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.71.0)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.15.2)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-32.0.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (9.1.2)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.0.2)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb) (3.10.16)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.23.0)\n",
            "Collecting starlette<0.46.0,>=0.40.0 (from fastapi==0.115.9->chromadb)\n",
            "  Downloading starlette-0.45.3-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (24.2)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.24.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.32.3)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.4.0)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.18)\n",
            "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.6.1)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.32.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.32.1-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting opentelemetry-proto==1.32.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.32.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-instrumentation-asgi==0.53b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.53b1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting opentelemetry-instrumentation==0.53b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation-0.53b1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.53b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.53b1)\n",
            "Collecting opentelemetry-util-http==0.53b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_util_http-0.53b1-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation==0.53b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.17.2)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.53b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.4.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (2.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.13.2->chromadb) (0.30.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-1.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "Downloading chromadb-1.0.7-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m69.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chroma_hnswlib-0.7.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.115.9-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kubernetes-32.0.1-py2.py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.21.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.32.1-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.32.1-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.32.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.53b1-py3-none-any.whl (12 kB)\n",
            "Downloading opentelemetry_instrumentation-0.53b1-py3-none-any.whl (30 kB)\n",
            "Downloading opentelemetry_instrumentation_asgi-0.53b1-py3-none-any.whl (16 kB)\n",
            "Downloading opentelemetry_util_http-0.53b1-py3-none-any.whl (7.3 kB)\n",
            "Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading posthog-4.0.1-py2.py3-none-any.whl (92 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading durationpy-0.9-py3-none-any.whl (3.5 kB)\n",
            "Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.45.3-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (454 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53800 sha256=cf9b472ef78c5d9939df08c93f727555681e865b01b31e1dfebec967de292743\n",
            "  Stored in directory: /root/.cache/pip/wheels/a3/01/bd/4c40ceb9d5354160cb186dcc153360f4ab7eb23e2b24daf96d\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, durationpy, uvloop, overrides, opentelemetry-util-http, opentelemetry-proto, mmh3, humanfriendly, httptools, chroma-hnswlib, bcrypt, backoff, asgiref, watchfiles, starlette, posthog, opentelemetry-exporter-otlp-proto-common, coloredlogs, onnxruntime, kubernetes, fastapi, opentelemetry-instrumentation, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, chromadb\n",
            "  Attempting uninstall: starlette\n",
            "    Found existing installation: starlette 0.46.2\n",
            "    Uninstalling starlette-0.46.2:\n",
            "      Successfully uninstalled starlette-0.46.2\n",
            "  Attempting uninstall: fastapi\n",
            "    Found existing installation: fastapi 0.115.12\n",
            "    Uninstalling fastapi-0.115.12:\n",
            "      Successfully uninstalled fastapi-0.115.12\n",
            "Successfully installed asgiref-3.8.1 backoff-2.2.1 bcrypt-4.3.0 chroma-hnswlib-0.7.6 chromadb-1.0.7 coloredlogs-15.0.1 durationpy-0.9 fastapi-0.115.9 httptools-0.6.4 humanfriendly-10.0 kubernetes-32.0.1 mmh3-5.1.0 onnxruntime-1.21.1 opentelemetry-exporter-otlp-proto-common-1.32.1 opentelemetry-exporter-otlp-proto-grpc-1.32.1 opentelemetry-instrumentation-0.53b1 opentelemetry-instrumentation-asgi-0.53b1 opentelemetry-instrumentation-fastapi-0.53b1 opentelemetry-proto-1.32.1 opentelemetry-util-http-0.53b1 overrides-7.7.0 posthog-4.0.1 pypika-0.48.9 starlette-0.45.3 uvloop-0.21.0 watchfiles-1.0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import JSONLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma"
      ],
      "metadata": {
        "id": "q0YoWjHaam7h"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore = None\n",
        "embeddings = None\n",
        "if json_filename and os.environ.get('GOOGLE_API_KEY'):\n",
        "    print(f\"\\n--- Setting up LangChain RAG pipeline for {json_filename} ---\")\n",
        "\n",
        "    def metadata_func(record: dict, metadata: dict) -> dict:\n",
        "        metadata[\"post_id\"] = record.get(\"id\", \"N/A\")\n",
        "        metadata[\"url\"] = record.get(\"url\", \"N/A\")\n",
        "        metadata[\"score\"] = record.get(\"score\", 0)\n",
        "\n",
        "        return metadata\n",
        "\n",
        "\n",
        "    loader = JSONLoader(\n",
        "        file_path=json_filename,\n",
        "        jq_schema='.[ ] | {content: (\"Title: \" + .title + \"\\nBody: \" + (.body // \"\")), id: .id, url: .url, score: .score}', # Extract content and metadata directly\n",
        "        content_key=\"content\",\n",
        "        metadata_func=metadata_func,\n",
        "        text_content=False\n",
        "    )\n",
        "\n",
        "\n",
        "    try:\n",
        "        print(\"Loading documents via LangChain JSONLoader...\")\n",
        "        documents = loader.load()\n",
        "        print(f\"Loaded {len(documents)} documents initially.\")\n",
        "\n",
        "        documents = [doc for doc in documents if doc.page_content.strip() and doc.page_content != \"Title: \\nBody: \"]\n",
        "        print(f\"Filtered down to {len(documents)} documents with content.\")\n",
        "\n",
        "        if documents:\n",
        "            print(\"\\nSample Document 1 Metadata:\", documents[0].metadata)\n",
        "\n",
        "\n",
        "\n",
        "        print(\"\\nInitializing text splitter...\")\n",
        "        text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=3000,\n",
        "            chunk_overlap=200\n",
        "        )\n",
        "\n",
        "\n",
        "        print(\"Splitting documents into chunks...\")\n",
        "        docs_split = text_splitter.split_documents(documents)\n",
        "        print(f\"Split into {len(docs_split)} chunks.\")\n",
        "\n",
        "        if docs_split:\n",
        "\n",
        "           print(\"\\nInitializing Google Generative AI Embeddings...\")\n",
        "\n",
        "           embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\") # Or \"models/embedding-001\"\n",
        "\n",
        "\n",
        "           print(\"Creating Chroma vector store and embedding chunks...\")\n",
        "\n",
        "           vectorstore = Chroma.from_documents(\n",
        "               documents=docs_split,\n",
        "               embedding=embeddings,\n",
        "\n",
        "           )\n",
        "           print(\"Chroma vector store created successfully.\")\n",
        "        else:\n",
        "            print(\"No document chunks to process after splitting. Vector store not created.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during LangChain processing: {e}\")\n",
        "        traceback.print_exc()\n",
        "\n",
        "else:\n",
        "    if not json_filename:\n",
        "        print(\"Skipping LangChain setup: JSON file not uploaded.\")\n",
        "    else:\n",
        "        print(\"Skipping LangChain setup: Google API Key not configured.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRSLspnXYGCi",
        "outputId": "7cb8443c-689a-439c-c079-e70ff85314b7"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Setting up LangChain RAG pipeline for wallstreetbets_week_250posts_20250425_183349 (1).json ---\n",
            "Loading documents via LangChain JSONLoader...\n",
            "Loaded 250 documents initially.\n",
            "Filtered down to 250 documents with content.\n",
            "\n",
            "Sample Document 1 Metadata: {'source': '/content/wallstreetbets_week_250posts_20250425_183349 (1).json', 'seq_num': 1, 'post_id': '1k5da53', 'url': 'https://i.redd.it/2vaajd0mgfwe1.png', 'score': 48717}\n",
            "\n",
            "Initializing text splitter...\n",
            "Splitting documents into chunks...\n",
            "Split into 282 chunks.\n",
            "\n",
            "Initializing Google Generative AI Embeddings...\n",
            "Creating Chroma vector store and embedding chunks...\n",
            "Chroma vector store created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def retrieve_relevant_context_langchain(query: str) -> dict:\n",
        "    \"\"\"\n",
        "    Retrieves relevant document chunks from the LangChain vector store\n",
        "    based on semantic similarity to the user's query. This tool searches\n",
        "    an indexed knowledge base of Reddit posts.\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- LangChain Retrieval Function called with query: '{query}' ---\")\n",
        "    global vectorstore\n",
        "\n",
        "    if vectorstore is None:\n",
        "        print(\"  [Function Error] LangChain vector store is not initialized.\")\n",
        "\n",
        "        return {\"status\": \"error\", \"message\": \"Vector store not ready.\", \"context\": \"\"}\n",
        "\n",
        "\n",
        "    try:\n",
        "\n",
        "        print(f\"  Performing similarity search (k=3)...\")\n",
        "\n",
        "        retrieved_docs = vectorstore.similarity_search(query, k=3)\n",
        "\n",
        "        if not retrieved_docs:\n",
        "            print(\"  [Function Info] No relevant documents found.\")\n",
        "            return {\"status\": \"success\", \"message\": \"No relevant context found.\", \"context\": \"\"}\n",
        "        else:\n",
        "\n",
        "            context_list = []\n",
        "            print(f\"  [Function Info] Retrieved {len(retrieved_docs)} chunks.\")\n",
        "            for i, doc in enumerate(retrieved_docs):\n",
        "\n",
        "                context_chunk = (\n",
        "                    f\"--- Context Chunk {i+1} ---\\n\"\n",
        "                    f\"Post ID: {doc.metadata.get('post_id', 'N/A')}\\n\"\n",
        "\n",
        "                    f\"Content: {doc.page_content}\"\n",
        "                )\n",
        "                context_list.append(context_chunk)\n",
        "\n",
        "            context_string = \"\\n\\n\".join(context_list)\n",
        "\n",
        "            print(f\"  [Function Success] Formatted context from {len(retrieved_docs)} chunks.\")\n",
        "\n",
        "            return {\"status\": \"success\", \"message\": \"Context retrieved.\", \"context\": context_string}\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  [Function Error] Error during LangChain retrieval: {e}\")\n",
        "        traceback.print_exc()\n",
        "        return {\"status\": \"error\", \"message\": f\"An error occurred during retrieval: {str(e)}\", \"context\": \"\"}\n",
        "\n",
        "\n",
        "if callable(retrieve_relevant_context_langchain):\n",
        "    print(\"\\nRetrieval function 'retrieve_relevant_context_langchain' is defined.\")\n",
        "else:\n",
        "     print(\"\\nError: 'retrieve_relevant_context_langchain' function not defined correctly.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MnLbVDobKLH",
        "outputId": "eef39a16-19e4-4a31-84b7-a5a0b6679357"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Retrieval function 'retrieve_relevant_context_langchain' is defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model name\n",
        "ADK_MODEL_NAME = 'gemini-2.0-flash'\n",
        "\n",
        "\n",
        "# Agent instructions - updated to reflect function name change if needed and context format\n",
        "rag_agent_instruction = \"\"\"You are a factual Q&A assistant answering questions about Reddit posts. Your task is to answer user questions based *only* on the context provided by the 'retrieve_relevant_context_langchain' function. Follow these steps precisely:\n",
        "1.  When you receive a user query, immediately call the 'retrieve_relevant_context_langchain' function with the exact user query to find relevant information from the indexed Reddit posts.\n",
        "2.  Examine the result from the function. The 'context' field in the returned dictionary may contain multiple chunks of text identified by '--- Context Chunk X ---' and including metadata like 'Post ID'.\n",
        "3.  If the function returns context (the 'context' field is not empty and status is 'success'), synthesize an answer based *solely* on the information present in ALL the provided context chunks. Start your response with: \"Based on the provided information: \". Do not add any information not present in the context. You can reference Post IDs if helpful.\n",
        "4.  If the function returns a message indicating no relevant context was found (e.g., the 'context' field is empty or status is not 'success'), respond *exactly* with: \"I could not find information about that in the indexed Reddit posts.\"\n",
        "5.  Do not use your general knowledge or any information outside the explicitly provided context from the function. If the function returns an error status, state that you encountered an issue retrieving information.\n",
        "\"\"\"\n",
        "\n",
        "rag_agent = None\n",
        "vectorstore_ready = 'vectorstore' in globals() and vectorstore is not None\n",
        "if os.environ.get('GOOGLE_API_KEY') and vectorstore_ready:\n",
        "    try:\n",
        "        rag_agent = Agent(\n",
        "            name=\"hybrid_rag_agent_v2\",\n",
        "            model=ADK_MODEL_NAME,\n",
        "            # Agent description\n",
        "            description=\"Answers questions using RAG based on Reddit posts, calling a retrieval function that uses LangChain internally.\",\n",
        "            instruction=rag_agent_instruction,\n",
        "            # Pass the raw function object in the list\n",
        "            tools=[retrieve_relevant_context_langchain],\n",
        "        )\n",
        "        print(f\"ADK Agent '{rag_agent.name}' created successfully with model '{ADK_MODEL_NAME}'.\")\n",
        "        print(f\"Agent Tools: {[tool.__name__ for tool in rag_agent.tools]}\") # Verify tool registration\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR creating ADK Agent (Model: {ADK_MODEL_NAME}): {e}\")\n",
        "        traceback.print_exc()\n",
        "else:\n",
        "    if not os.environ.get('GOOGLE_API_KEY'):\n",
        "        print(\"Agent creation skipped: Google API Key is not configured.\")\n",
        "    elif not vectorstore_ready:\n",
        "        print(\"Agent creation skipped: LangChain Vector Store is not ready (check Cell 5).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iR0hjA9tbVZJ",
        "outputId": "90361471-e0f1-4c46-c238-a2c7d1bc1c8e"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ADK Agent 'hybrid_rag_agent_v2' created successfully with model 'gemini-2.0-flash'.\n",
            "Agent Tools: ['retrieve_relevant_context_langchain']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adk_runner = None\n",
        "adk_session_service = None\n",
        "\n",
        "if rag_agent:\n",
        "    adk_session_service = InMemorySessionService()\n",
        "    adk_runner = Runner(\n",
        "        agent=rag_agent,\n",
        "        app_name=\"SimpleRAGTestApp\",\n",
        "        session_service=adk_session_service,\n",
        "    )\n",
        "    print(\"ADK Runner and Session Service initialized.\")\n",
        "else:\n",
        "    print(\"ADK Runner initialization skipped because the Agent was not created.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5Pwk0-qbqOR",
        "outputId": "b0024ae8-e016-47d1-a75b-f9d7c2da74bf"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ADK Runner and Session Service initialized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "async def ask_rag_agent(query: str):\n",
        "    \"\"\"\n",
        "    Sends a query to the configured RAG agent, verifies session creation,\n",
        "    and prints the interaction flow and final response.\n",
        "    Returns the final text response from the agent.\n",
        "    \"\"\"\n",
        "    if not adk_runner or not adk_session_service:\n",
        "        print(\"ERROR: ADK Runner or Session Service is not initialized. Cannot run query.\")\n",
        "        return \"Error: ADK components not ready.\"\n",
        "\n",
        "    session_id = f\"rag_test_session_{os.urandom(4).hex()}\"\n",
        "    user_id = \"colab_user_test\" # Consistent user ID\n",
        "    # Get app_name from the runner object\n",
        "    app_name = adk_runner.app_name if adk_runner else \"SimpleRAGTestApp\" # Fallback name if needed\n",
        "\n",
        "    print(f\"\\n=================================================\")\n",
        "    print(f\"--- Starting Query for: '{query}' (Session: {session_id}) ---\")\n",
        "    print(f\"--- App: {app_name}, User: {user_id} ---\")\n",
        "    print(f\"=================================================\")\n",
        "\n",
        "    final_response = \"Agent execution did not yield a final text response.\"\n",
        "    session_verified = False # Flag to track if session seems okay before running\n",
        "\n",
        "    try:\n",
        "        # --- Step 1: Attempt to Create Session ---\n",
        "        print(f\"  Attempting to create session: {session_id}\")\n",
        "        adk_session_service.create_session(\n",
        "            app_name=app_name,\n",
        "            user_id=user_id,\n",
        "            session_id=session_id\n",
        "        )\n",
        "        print(f\"  ADK 'create_session' called for {session_id} (No immediate error thrown).\")\n",
        "\n",
        "        # --- Step 2: Attempt to Verify Session Existence (Corrected Call) ---\n",
        "        try:\n",
        "            print(f\"  Verifying session {session_id} using get_session...\")\n",
        "            # *** CORRECTED CALL: Added app_name and user_id ***\n",
        "            retrieved_session = adk_session_service.get_session(\n",
        "                app_name=app_name,\n",
        "                user_id=user_id,\n",
        "                session_id=session_id\n",
        "            )\n",
        "            # Check if the retrieved session object is valid (might be specific to ADK version)\n",
        "            # For InMemorySessionService, it likely returns the session state dict or raises error\n",
        "            if retrieved_session is not None: # Basic check\n",
        "                 print(f\"  VERIFICATION SUCCESS: Successfully retrieved session {session_id} using get_session().\")\n",
        "                 session_verified = True\n",
        "            else:\n",
        "                 print(f\"  VERIFICATION WARNING: get_session({session_id}) returned None.\")\n",
        "        except Exception as get_e:\n",
        "            print(f\"  VERIFICATION ERROR: Failed to retrieve session {session_id} using get_session(): {get_e}\")\n",
        "            # If get_session fails, the runner likely won't find it either.\n",
        "\n",
        "        # --- Step 3: Run Agent (Only if Session Verification Succeeded) ---\n",
        "        if session_verified:\n",
        "            print(f\"  Proceeding to call adk_runner.run_async for verified session {session_id}...\")\n",
        "            content = adk_types.Content(role='user', parts=[adk_types.Part(text=query)])\n",
        "\n",
        "            async for event in adk_runner.run_async(user_id=user_id, session_id=session_id, new_message=content):\n",
        "                # ... (event processing loop - unchanged) ...\n",
        "                event_type = type(event).__name__\n",
        "                print(f\"    [ADK Event]: {event_type}\")\n",
        "\n",
        "                if hasattr(event, 'tool_call') and event.tool_call:\n",
        "                    tool_name = getattr(event.tool_call, 'name', 'N/A')\n",
        "                    tool_args = getattr(event.tool_call, 'args', {})\n",
        "                    print(f\"      -> Agent requesting tool: '{tool_name}'\")\n",
        "                    print(f\"         Args: {tool_args}\")\n",
        "\n",
        "                if hasattr(event, 'content') and event.content:\n",
        "                    content_role = getattr(event.content, 'role', 'N/A')\n",
        "                    if hasattr(event.content, 'parts'):\n",
        "                        for i, part in enumerate(event.content.parts):\n",
        "                            if hasattr(part, 'function_response') and part.function_response:\n",
        "                                fr_name = getattr(part.function_response, 'name', 'N/A')\n",
        "                                fr_resp = getattr(part.function_response, 'response', {})\n",
        "                                print(f\"      -> Function Response from '{fr_name}' received.\")\n",
        "                            if hasattr(part, 'text') and part.text:\n",
        "                                if content_role == 'model':\n",
        "                                     print(f\"      -> Model Text Response Part: '{part.text[:150]}{'...' if len(part.text)>150 else ''}'\")\n",
        "                                     final_response = part.text\n",
        "\n",
        "            print(f\"\\n--- Agent execution loop finished for Session: {session_id} ---\")\n",
        "\n",
        "        else:\n",
        "             print(f\"--- Agent execution skipped because session {session_id} could not be verified ---\")\n",
        "             final_response = \"Error: Failed to create or verify ADK session.\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR during agent execution/session handling for {session_id}: {e}\")\n",
        "        if \"Session not found\" in str(e) and session_verified:\n",
        "             print(\"  >>> CRITICAL: Got 'Session not found' during run_async even after verification passed!\")\n",
        "        traceback.print_exc()\n",
        "        final_response = f\"An error occurred: {str(e)}\"\n",
        "\n",
        "    finally:\n",
        "        # --- Step 4: Clean up the session ---\n",
        "        try:\n",
        "            if adk_session_service and adk_runner:\n",
        "                print(f\"  Attempting to delete session {session_id}...\")\n",
        "                adk_session_service.delete_session(\n",
        "                    app_name=app_name, # Use correct app_name\n",
        "                    user_id=user_id,\n",
        "                    session_id=session_id\n",
        "                )\n",
        "                print(f\"  Session {session_id} delete requested.\")\n",
        "        except Exception as del_e:\n",
        "            print(f\"  Info/Warning during session deletion for {session_id}: {del_e}\")\n",
        "\n",
        "    print(f\"\\n>>> Final Response: {final_response}\")\n",
        "    print(f\"=================================================\")\n",
        "    return final_response"
      ],
      "metadata": {
        "id": "-bn2cduVbwFD"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define an async function to run the tests\n",
        "async def run_all_tests():\n",
        "    if not adk_runner:\n",
        "        print(\"Cannot run tests - ADK Runner is not initialized.\")\n",
        "        return\n",
        "\n",
        "    # Test Case 1: Query likely in KB\n",
        "    await ask_rag_agent(\"What are the stocks that are being talked about?\")\n",
        "\n",
        "    await ask_rag_agent(\"Which stalks seem to have the most positive sentiment in the entire post list?\")\n",
        "\n",
        "\n",
        "\n",
        "# --- IMPORTANT ---\n",
        "# In Colab/Jupyter, you run the async function like this:\n",
        "await run_all_tests()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvQEYJlEbxI0",
        "outputId": "e88a41d4-4296-486a-beb0-9d4283bfa16a"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=================================================\n",
            "--- Starting Query for: 'What are the stocks that are being talked about?' (Session: rag_test_session_165a5218) ---\n",
            "--- App: SimpleRAGTestApp, User: colab_user_test ---\n",
            "=================================================\n",
            "  Attempting to create session: rag_test_session_165a5218\n",
            "  ADK 'create_session' called for rag_test_session_165a5218 (No immediate error thrown).\n",
            "  Verifying session rag_test_session_165a5218 using get_session...\n",
            "  VERIFICATION SUCCESS: Successfully retrieved session rag_test_session_165a5218 using get_session().\n",
            "  Proceeding to call adk_runner.run_async for verified session rag_test_session_165a5218...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'],returning concatenated text result from text parts,check out the non text parts for full response from model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [ADK Event]: Event\n",
            "\n",
            "--- LangChain Retrieval Function called with query: 'What are the stocks that are being talked about?' ---\n",
            "  Performing similarity search (k=3)...\n",
            "  [Function Info] Retrieved 3 chunks.\n",
            "  [Function Success] Formatted context from 3 chunks.\n",
            "    [ADK Event]: Event\n",
            "      -> Function Response from 'retrieve_relevant_context_langchain' received.\n",
            "    [ADK Event]: Event\n",
            "      -> Model Text Response Part: 'Based on the provided information: The stocks being talked about are TSLA (Tesla), domestic lithium refinery stocks, NOW, and GOOG. Post ID 1k3ou80 di...'\n",
            "\n",
            "--- Agent execution loop finished for Session: rag_test_session_165a5218 ---\n",
            "  Attempting to delete session rag_test_session_165a5218...\n",
            "  Session rag_test_session_165a5218 delete requested.\n",
            "\n",
            ">>> Final Response: Based on the provided information: The stocks being talked about are TSLA (Tesla), domestic lithium refinery stocks, NOW, and GOOG. Post ID 1k3ou80 discusses Tesla earnings. Post ID 1k57wvu discusses domestic lithium refinery stocks. Post ID 1k6vuph mentions NOW and GOOG.\n",
            "\n",
            "=================================================\n",
            "\n",
            "=================================================\n",
            "--- Starting Query for: 'Which stalks seem to have the most positive sentiment in the entire post list?' (Session: rag_test_session_84ce2342) ---\n",
            "--- App: SimpleRAGTestApp, User: colab_user_test ---\n",
            "=================================================\n",
            "  Attempting to create session: rag_test_session_84ce2342\n",
            "  ADK 'create_session' called for rag_test_session_84ce2342 (No immediate error thrown).\n",
            "  Verifying session rag_test_session_84ce2342 using get_session...\n",
            "  VERIFICATION SUCCESS: Successfully retrieved session rag_test_session_84ce2342 using get_session().\n",
            "  Proceeding to call adk_runner.run_async for verified session rag_test_session_84ce2342...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'],returning concatenated text result from text parts,check out the non text parts for full response from model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [ADK Event]: Event\n",
            "\n",
            "--- LangChain Retrieval Function called with query: 'Which stalks seem to have the most positive sentiment in the entire post list?' ---\n",
            "  Performing similarity search (k=3)...\n",
            "  [Function Info] Retrieved 3 chunks.\n",
            "  [Function Success] Formatted context from 3 chunks.\n",
            "    [ADK Event]: Event\n",
            "      -> Function Response from 'retrieve_relevant_context_langchain' received.\n",
            "    [ADK Event]: Event\n",
            "      -> Model Text Response Part: 'Based on the provided information: Post ID 1k4jh6e mentions a $3.8k profit on NFE puts. Post ID 1k7r9id refers to a $23k gain on GOOG. Post ID 1k5znh7...'\n",
            "\n",
            "--- Agent execution loop finished for Session: rag_test_session_84ce2342 ---\n",
            "  Attempting to delete session rag_test_session_84ce2342...\n",
            "  Session rag_test_session_84ce2342 delete requested.\n",
            "\n",
            ">>> Final Response: Based on the provided information: Post ID 1k4jh6e mentions a $3.8k profit on NFE puts. Post ID 1k7r9id refers to a $23k gain on GOOG. Post ID 1k5znh7 discusses hedging gambles and turning losses into a 1k profit.\n",
            "\n",
            "=================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QzSGnOQ8byqY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}